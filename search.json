[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Though I was born and raised in Virginia, my career in the Air Force eventually brought me to Texas, where I have lived for the past five years while continuing to grow both personally and professionally. I’m currently pursuing a Master’s degree in Social Data Analytics & Research (graduating May 2026) because I believe in the importance of data-driven decision making.\nBut how did I find myself at the doorstep of data? Over the last 8+ years, I’ve built experience in behavioral health, program management, and policy evaluation through work in community, hospital, military, and government settings. Along the way, I realized that while systems and processes are designed to improve quality of life, many don’t always function as intended. And when proposals for new or revised programs are made, they often require evidence and research to guide the best path forward. For me, the most objective and balanced way forward is through the use of data, while also recognizing its limitations and the importance of having people in these spaces who can apply it effectively.\nThrough this education, I aspire to bridge clinical insight with strategic thinking to improve how support services are delivered. My goal is to ensure that programs not only exist, but actually work as intended, and when they don’t, that I have the skills to redesign them in ways that create meaningful, sustainable change."
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\n\nUrban planning and neighborhood development\nSocial determinants of health\nData-driven approaches to improving community outcomes\nGeographic Information Systems (GIS) for spatial analysis"
  },
  {
    "objectID": "about.html#certifications-and-licenses",
    "href": "about.html#certifications-and-licenses",
    "title": "About",
    "section": "Certifications and Licenses",
    "text": "Certifications and Licenses\n\nLicensed Clinical Social Worker (LCSW-Supervisor) – Texas & Virginia\nProject Management Professional (PMP)\nBoard Certified Diplomate (BCD), Clinical Social Work"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nLinkedIn: https://linkedin.com/in/tyerelldotson"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "My name is Tyerell Dotson and I’m a graduate student for UTD’s Social Data Analytics and Research program. This website was created using RStudio Quarto for my Methods of Data Collection and Production course. It is intended to serve as a place to store and share data-related coursework and future professional projects. Perhaps, it will also be useful in the future for collaboration with other professionals with similar interests.\nCheck the navigation bar above to learn more about me, access my resume, or course-related content."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "For this comparison, I picked July 1 – November 30, 2024 because it lines up with the big campaign events that drove public interest. In June, Biden took a lot of criticism about his fitness for office, and in July he stepped down and endorsed Kamala Harris. On the Republican side, Trump accepted the party nomination not long after the alleged assassination attempt. November is significant due to Election Day. Put together, this stretch of time is the most relevant for comparing search interest in Trump and Harris during the 2024 election.\n\n\n\nGoogle Trends provides a readable interface and a CSV for comparisons: a date column and one column per search term with values on a 0–100 scale.\n\nThe scale equals relative popularity within the selected time & place; 100 is the peak in that window, and other values are scaled to it. Meaning, not raw number of searches but instead, how popular a term is compared to all Google searches during the set time and location. As mentioned, the highest point in the chart is set to 100, and everything else is scaled against that peak. A “50” means the term was half as popular as it was at its peak, not that it had 50 searches. Intervals are automatically chosen by Google Trends based on the date range (hourly, daily, weekly, or monthly). For this search, the intervals are daily.\n\nweb &lt;- read.csv(\"Data/trumpvskamalaharris.csv\", skip = 2, check.names = FALSE)\nnames(web)[1] &lt;- \"date\"; web$date &lt;- as.Date(web$date)\nfor (j in 2:ncol(web)) { web[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", web[[j]])) }\nhead(web, 10)\n\n         date trump: (United States) kamala harris: (United States)\n1  2024-07-01                      4                            0.5\n2  2024-07-02                      4                            0.5\n3  2024-07-03                      3                            0.5\n4  2024-07-04                      3                            0.5\n5  2024-07-05                      3                            0.5\n6  2024-07-06                      3                            0.5\n7  2024-07-07                      3                            0.5\n8  2024-07-08                      2                            0.5\n9  2024-07-09                      3                            0.5\n10 2024-07-10                      3                            0.5\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(web), ncol(web)))\n\nPreview shown · Full file: 153 rows × 3 columns\n\n\n\n\n\nAdditionally, I pulled the same data directly into RStudio using the gtrendsR package and saved it into CSV, R Data, and RDS formats.\n\nlibrary(gtrendsR)\n\nkw &lt;- c(\"Trump\", \"Kamala Harris\")\ngeo &lt;- \"US\"\nwindow &lt;- \"2024-07-01 2024-11-30\"\n\ngt &lt;- gtrends(keyword = kw, geo = geo, time = window, onlyInterest = TRUE)\n\nwrite.csv(gt$interest_over_time, \"Data/gtrends_trump_harris_2024_iot.csv\", row.names = FALSE)\nsave(gt,  file = \"Data/gtrends_trump_harris_2024.RData\")\nsaveRDS(gt, file = \"Data/gtrends_trump_harris_2024.rds\")\n\n\n\n\nI downloaded each search term separately from Google Trends as CSV files and pulled the same data in RStudio with the gtrendsr package, which gave me some extra practice using code to collect and work with data.\nTrump\n\ntrump &lt;- read.csv(\"Data/trump.csv\", skip = 2, check.names = FALSE)\nnames(trump)[1] &lt;- \"date\"; trump$date &lt;- as.Date(trump$date)\nfor (j in 2:ncol(trump)) { trump[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", trump[[j]])) }\nhead(trump, 10)\n\n         date trump: (United States)\n1  2024-07-01                      4\n2  2024-07-02                      4\n3  2024-07-03                      3\n4  2024-07-04                      3\n5  2024-07-05                      3\n6  2024-07-06                      3\n7  2024-07-07                      3\n8  2024-07-08                      2\n9  2024-07-09                      3\n10 2024-07-10                      3\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(trump), ncol(trump)))\n\nPreview shown · Full file: 153 rows × 2 columns\n\n\nKamala Harris\n\nkamala &lt;- read.csv(\"Data/kamalaharris.csv\", skip = 2, check.names = FALSE)\nnames(kamala)[1] &lt;- \"date\"; kamala$date &lt;- as.Date(kamala$date)\nfor (j in 2:ncol(kamala)) { kamala[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", kamala[[j]])) }\nhead(kamala, 10)\n\n         date kamala harris: (United States)\n1  2024-07-01                              1\n2  2024-07-02                              1\n3  2024-07-03                              2\n4  2024-07-04                              2\n5  2024-07-05                              2\n6  2024-07-06                              1\n7  2024-07-07                              1\n8  2024-07-08                              1\n9  2024-07-09                              1\n10 2024-07-10                              1\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(kamala), ncol(kamala)))\n\nPreview shown · Full file: 153 rows × 2 columns\n\n\nElection\n\nelection &lt;- read.csv(\"Data/election.csv\", skip = 2, check.names = FALSE)\nnames(election)[1] &lt;- \"date\"; election$date &lt;- as.Date(election$date)\nfor (j in 2:ncol(election)) { election[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", election[[j]])) }\nhead(election, 10)\n\n         date election: (United States)\n1  2024-07-01                       0.5\n2  2024-07-02                       0.5\n3  2024-07-03                       0.5\n4  2024-07-04                       0.5\n5  2024-07-05                       0.5\n6  2024-07-06                       0.5\n7  2024-07-07                       0.5\n8  2024-07-08                       0.5\n9  2024-07-09                       0.5\n10 2024-07-10                       0.5\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(election), ncol(election)))\n\nPreview shown · Full file: 153 rows × 2 columns\n\n\n\n\n\nI used both the Google Trends website and the gtrendsR package to collect data. Both methods provide the same output but there is some differences in terms of simplicity. Downloading CSVs from the website was straightforward and gave me quick access to the search results. Using gtrendsR took a little more effort since it required writing code, something I am just now learning how to do, but once the data was pulled into RStudio I could immediately save it in multiple formats and start analyzing it in the same environment. In other words, the website method is quick and user-friendly, while the RStudio method is more efficient because you’re able to do different things with it from the same application as opposed to pulling it from Google trends and then uploading somewhere else to use the data. Though the outcome is the same, the gtrendsR package adds flexibility once you’re comfortable with the code."
  },
  {
    "objectID": "assign02.html#comparing-trump-and-kamala-harris",
    "href": "assign02.html#comparing-trump-and-kamala-harris",
    "title": "Assignment 2",
    "section": "",
    "text": "For this comparison, I picked July 1 – November 30, 2024 because it lines up with the big campaign events that drove public interest. In June, Biden took a lot of criticism about his fitness for office, and in July he stepped down and endorsed Kamala Harris. On the Republican side, Trump accepted the party nomination not long after the alleged assassination attempt. November is significant due to Election Day. Put together, this stretch of time is the most relevant for comparing search interest in Trump and Harris during the 2024 election."
  },
  {
    "objectID": "assign02.html#how-to-read-the-data",
    "href": "assign02.html#how-to-read-the-data",
    "title": "Assignment 2",
    "section": "",
    "text": "Google Trends provides a readable interface and a CSV for comparisons: a date column and one column per search term with values on a 0–100 scale.\n\nThe scale equals relative popularity within the selected time & place; 100 is the peak in that window, and other values are scaled to it. Meaning, not raw number of searches but instead, how popular a term is compared to all Google searches during the set time and location. As mentioned, the highest point in the chart is set to 100, and everything else is scaled against that peak. A “50” means the term was half as popular as it was at its peak, not that it had 50 searches. Intervals are automatically chosen by Google Trends based on the date range (hourly, daily, weekly, or monthly). For this search, the intervals are daily.\n\nweb &lt;- read.csv(\"Data/trumpvskamalaharris.csv\", skip = 2, check.names = FALSE)\nnames(web)[1] &lt;- \"date\"; web$date &lt;- as.Date(web$date)\nfor (j in 2:ncol(web)) { web[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", web[[j]])) }\nhead(web, 10)\n\n         date trump: (United States) kamala harris: (United States)\n1  2024-07-01                      4                            0.5\n2  2024-07-02                      4                            0.5\n3  2024-07-03                      3                            0.5\n4  2024-07-04                      3                            0.5\n5  2024-07-05                      3                            0.5\n6  2024-07-06                      3                            0.5\n7  2024-07-07                      3                            0.5\n8  2024-07-08                      2                            0.5\n9  2024-07-09                      3                            0.5\n10 2024-07-10                      3                            0.5\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(web), ncol(web)))\n\nPreview shown · Full file: 153 rows × 3 columns"
  },
  {
    "objectID": "assign02.html#gtrendsr",
    "href": "assign02.html#gtrendsr",
    "title": "Assignment 2",
    "section": "",
    "text": "Additionally, I pulled the same data directly into RStudio using the gtrendsR package and saved it into CSV, R Data, and RDS formats.\n\nlibrary(gtrendsR)\n\nkw &lt;- c(\"Trump\", \"Kamala Harris\")\ngeo &lt;- \"US\"\nwindow &lt;- \"2024-07-01 2024-11-30\"\n\ngt &lt;- gtrends(keyword = kw, geo = geo, time = window, onlyInterest = TRUE)\n\nwrite.csv(gt$interest_over_time, \"Data/gtrends_trump_harris_2024_iot.csv\", row.names = FALSE)\nsave(gt,  file = \"Data/gtrends_trump_harris_2024.RData\")\nsaveRDS(gt, file = \"Data/gtrends_trump_harris_2024.rds\")"
  },
  {
    "objectID": "assign02.html#additional-searches",
    "href": "assign02.html#additional-searches",
    "title": "Assignment 2",
    "section": "",
    "text": "I downloaded each search term separately from Google Trends as CSV files and pulled the same data in RStudio with the gtrendsr package, which gave me some extra practice using code to collect and work with data.\nTrump\n\ntrump &lt;- read.csv(\"Data/trump.csv\", skip = 2, check.names = FALSE)\nnames(trump)[1] &lt;- \"date\"; trump$date &lt;- as.Date(trump$date)\nfor (j in 2:ncol(trump)) { trump[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", trump[[j]])) }\nhead(trump, 10)\n\n         date trump: (United States)\n1  2024-07-01                      4\n2  2024-07-02                      4\n3  2024-07-03                      3\n4  2024-07-04                      3\n5  2024-07-05                      3\n6  2024-07-06                      3\n7  2024-07-07                      3\n8  2024-07-08                      2\n9  2024-07-09                      3\n10 2024-07-10                      3\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(trump), ncol(trump)))\n\nPreview shown · Full file: 153 rows × 2 columns\n\n\nKamala Harris\n\nkamala &lt;- read.csv(\"Data/kamalaharris.csv\", skip = 2, check.names = FALSE)\nnames(kamala)[1] &lt;- \"date\"; kamala$date &lt;- as.Date(kamala$date)\nfor (j in 2:ncol(kamala)) { kamala[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", kamala[[j]])) }\nhead(kamala, 10)\n\n         date kamala harris: (United States)\n1  2024-07-01                              1\n2  2024-07-02                              1\n3  2024-07-03                              2\n4  2024-07-04                              2\n5  2024-07-05                              2\n6  2024-07-06                              1\n7  2024-07-07                              1\n8  2024-07-08                              1\n9  2024-07-09                              1\n10 2024-07-10                              1\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(kamala), ncol(kamala)))\n\nPreview shown · Full file: 153 rows × 2 columns\n\n\nElection\n\nelection &lt;- read.csv(\"Data/election.csv\", skip = 2, check.names = FALSE)\nnames(election)[1] &lt;- \"date\"; election$date &lt;- as.Date(election$date)\nfor (j in 2:ncol(election)) { election[[j]] &lt;- as.numeric(gsub(\"&lt;1\",\"0.5\", election[[j]])) }\nhead(election, 10)\n\n         date election: (United States)\n1  2024-07-01                       0.5\n2  2024-07-02                       0.5\n3  2024-07-03                       0.5\n4  2024-07-04                       0.5\n5  2024-07-05                       0.5\n6  2024-07-06                       0.5\n7  2024-07-07                       0.5\n8  2024-07-08                       0.5\n9  2024-07-09                       0.5\n10 2024-07-10                       0.5\n\ncat(sprintf(\"Preview shown · Full file: %d rows × %d columns\\n\", nrow(election), ncol(election)))\n\nPreview shown · Full file: 153 rows × 2 columns"
  },
  {
    "objectID": "assign02.html#collection-method-differences",
    "href": "assign02.html#collection-method-differences",
    "title": "Assignment 2",
    "section": "",
    "text": "I used both the Google Trends website and the gtrendsR package to collect data. Both methods provide the same output but there is some differences in terms of simplicity. Downloading CSVs from the website was straightforward and gave me quick access to the search results. Using gtrendsR took a little more effort since it required writing code, something I am just now learning how to do, but once the data was pulled into RStudio I could immediately save it in multiple formats and start analyzing it in the same environment. In other words, the website method is quick and user-friendly, while the RStudio method is more efficient because you’re able to do different things with it from the same application as opposed to pulling it from Google trends and then uploading somewhere else to use the data. Though the outcome is the same, the gtrendsR package adds flexibility once you’re comfortable with the code."
  },
  {
    "objectID": "podreflection.html",
    "href": "podreflection.html",
    "title": "Podcast Reflection - DataFramed",
    "section": "",
    "text": "Launching a Data Science Career in 2025\nBeing tasked with checking out this podcast, I wasn’t sure what to expect or if I’d even be interested in what it had to offer. But with a quick scroll, I found something that caught my eye, Breaking Into Data Science in 2025. While I’m not necessarily looking to become a data scientist, I was hoping to gain some insight into how to enter the world of data more broadly, and I walked away with some valuable information. I’ll break down my reflection into three parts: the relevancy of data-adjacent roles, the hiring process, and applying information.\nThe episode starts with a reference to an article I actually read several years ago about the attractiveness of data science roles and whether they still provide value to companies. I was glad to hear that, despite the evolution of AI, data science positions are still very much relevant, though the role has changed. As you’d imagine, AI helps data scientists work more efficiently by writing code, creating reports or documentation, and gathering information or data from different sources. AI hasn’t replaced data-related positions, it’s created more opportunities; but how?In the past, data professionals were often seen as Swiss-army knives with skills across statistics, coding, analysis, and business. Now, AI increases productivity and supports the development of specialists who can maximize their expertise as opposed to the field being exclusively for those with very specific education backgrounds.\nThe podcast then shifts to what it takes to become a data scientist and how to land a role. The guest shared that it took them six years of interviews before landing their first data scientist job, but along the way they held several analyst positions. What stood out to me was the recognition that while data scientists used to come mostly from math-heavy degrees, today there are master’s and PhD programs dedicated specifically to data, which provides a clearer path into the profession. Still, it’s not just about education. Hard and soft skills matter, like knowing tools such as Python or SQL, being a strong communicator, adapting to change, and working well on a team. I also found it useful that resumes should highlight projects using a structure of what you did, how you did it, and most importantly, what the outcome was. That approach is something I’m very much familiar with since the military drills the same structure into annual evaluations.\nOverall, this podcast gave me a clearer picture of how data-related roles are evolving and which technical skills I need to sharpen (Python and SQL are definitely on my list) to stay competitive. Because my background is rooted in the social work, the soft skills are already something that I’ve mastered and I think will serve me well as I enter the field. It was also encouraging to hear AI framed as a tool to use, rather than something to fear. I left the podcast motivated and plan to adopt the 10–20% rule, meaning that 10 to 20% of my job should always be about learning something new."
  },
  {
    "objectID": "assign04.html",
    "href": "assign04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Web Scraping\n\nurl_fx &lt;- \"https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves\"\n\nfx_raw &lt;- read_html(url_fx) |&gt;\n  html_elements(\"table.wikitable\") |&gt;\n  (\\(x) html_table(x, fill = TRUE))() |&gt;\n  (\\(x) x[[1]])()\n\nfx &lt;- fx_raw |&gt;\n  janitor::remove_empty(c(\"rows\",\"cols\")) |&gt;\n  janitor::clean_names()\n\nto_num &lt;- function(x) {\n  x &lt;- as.character(x)\n  x &lt;- gsub(\"[^0-9\\\\-\\\\.]\", \"\", x)  # strip commas, spaces, footnotes, symbols\n  x[!grepl(\"\\\\d\", x)] &lt;- NA         # if no digits at all, set NA\n  suppressWarnings(readr::parse_number(x))\n}\n\nfx_clean &lt;- fx |&gt;\n  dplyr::transmute(\n    country   = country_as_recognized_by_the_u_n,\n    continent = continent,\n    reserves_including_gold_musd = to_num(foreign_exchange_reserves),\n    reserves_excluding_gold_musd = to_num(foreign_exchange_reserves_3),\n    as_of = Sys.Date()\n  ) |&gt;\n  dplyr::filter(\n    !is.na(country), country != \"\",\n    !is.na(reserves_including_gold_musd) | !is.na(reserves_excluding_gold_musd)\n  ) |&gt;\n  head(15)\n\nfx_clean\n\n# A tibble: 15 × 5\n   country    continent reserves_including_g…¹ reserves_excluding_g…² as_of     \n   &lt;chr&gt;      &lt;chr&gt;                      &lt;dbl&gt;                  &lt;dbl&gt; &lt;date&gt;    \n 1 China      Asia                     3643149                3389306 2025-11-16\n 2 Japan      Asia                     1324210                1230940 2025-11-16\n 3 Switzerla… Europe                   1007710                 897295 2025-11-16\n 4 Russia     Europe/A…                 742400                 434487 2025-11-16\n 5 India      Asia                      695356                 589820 2025-11-16\n 6 Taiwan     Asia                      597430                 544300 2025-11-16\n 7 Saudi Ara… Asia                      434547                 434116 2025-11-16\n 8 Hong Kong  Asia                      421400                 416216 2025-11-16\n 9 South Kor… Asia                      415700                 410900 2025-11-16\n10 Brazil     Americas                  388571                 344173 2025-11-16\n11 Singapore  Asia                      383721                 374809 2025-11-16\n12 Germany    Europe                    345338                  91071 2025-11-16\n13 Italy      Europe                    279631                  79064 2025-11-16\n14 France     Europe                    272693                  66641 2025-11-16\n15 United St… Americas                  253767                 242726 2025-11-16\n# ℹ abbreviated names: ¹​reserves_including_gold_musd,\n#   ²​reserves_excluding_gold_musd\n\n\n\n\n# A tibble: 15 × 4\n   country                  production_raw production_carat as_of     \n   &lt;chr&gt;                    &lt;chr&gt;                     &lt;dbl&gt; &lt;date&gt;    \n 1 Country                  Carats[1]                     1 2025-11-16\n 2 Russia                   37,322,794             37322794 2025-11-16\n 3 Botswana                 28,181,710             28181710 2025-11-16\n 4 Angola                   14,027,003             14027003 2025-11-16\n 5 Canada                   13,321,628             13321628 2025-11-16\n 6 DR Congo                 9,788,202               9788202 2025-11-16\n 7 South Africa             5,340,219               5340219 2025-11-16\n 8 Zimbabwe                 5,293,329               5293329 2025-11-16\n 9 Namibia                  2,319,489               2319489 2025-11-16\n10 Lesotho                  695,888                  695888 2025-11-16\n11 Sierra Leone             573,983                  573983 2025-11-16\n12 Tanzania                 373,679                  373679 2025-11-16\n13 Ghana                    332,790                  332790 2025-11-16\n14 Guinea                   113,379                  113379 2025-11-16\n15 Central African Republic 111,377                  111377 2025-11-16\n\n\n\n\n# A tibble: 15 × 3\n   country        exports_usd_thousands as_of     \n   &lt;chr&gt;                          &lt;dbl&gt; &lt;date&gt;    \n 1 Germany                    129088980 2025-11-16\n 2 Switzerland                110678748 2025-11-16\n 3 United States              102488068 2025-11-16\n 4 Ireland                     89815222 2025-11-16\n 5 Belgium                     84934569 2025-11-16\n 6 Italy                       55552143 2025-11-16\n 7 France                      40322238 2025-11-16\n 8 Netherlands                 38541479 2025-11-16\n 9 United Kingdom              29103095 2025-11-16\n10 Slovenia                    27196651 2025-11-16\n11 India                       23427774 2025-11-16\n12 Denmark                     21824168 2025-11-16\n13 Austria                     21211914 2025-11-16\n14 Spain                       18260920 2025-11-16\n15 China                       13702663 2025-11-16\n\n\n\n\nData Plan\nIf I were to acquire web data for research in the future, I would develop a data plan focused on ensuring accuracy, transparency, and ethical collection practices. This would include identifying reliable and authoritative websites, databases, or APIs that allow access to relevant information. I would also prioritize reproducibility by clearly documenting the data sources, collection methods, and any transformations made during cleaning or analysis. When using web scraping, I would comply with site policies, minimize bias, and verify the data’s validity before incorporating it into the study. The goal would be to gather structured data that supports the research question and can withstand academic scrutiny."
  },
  {
    "objectID": "assign05.html",
    "href": "assign05.html",
    "title": "Assignment 5 - Govt Data",
    "section": "",
    "text": "This assignment involved scraping data from the GovInfo website. I initially used the rvest package, which worked well for static sites like Wikipedia. However, I learned that rvest cannot interact with dynamic elements or trigger buttons that generate downloadable files. Because the GovInfo site requires exporting search results, this method could not be used to directly scrape the data.\n\n\nAfter reviewing the sample script, I realized the correct process was to export the search results in CSV or JSON format and then use R to download the linked PDFs. The CSV method only downloaded two PDFs, while switching to the JSON file improved the results but still didn’t reach the full number expected. Even after increasing the target from 10 to 20, only 14 files were downloaded.\nThe main difficulty was that the exported data did not fully match what appeared on the GovInfo website. Although most of the search results on the site showed a visible PDF button, some of those records either lacked a valid pdf link or contained empty fields in the export. This caused the script to skip certain rows or return fewer files than expected.\n\n\n\nThe exported data was usable as the files that were successfully downloaded all opened correctly as PDFs, showing that the links were valid. However, the exports included missing or invalid fields that limited the total number of successful downloads. The data provided enough structure, titles, dates, for further use, but would require cleanup and verification to ensure complete and accurate results.\n\n\n\nThe main area for improvement would be addressing the mismatch between the exported files and the actual search results displayed on the website. Cleaning the data to remove empty or invalid fields, verifying that each record includes a working link, and ensuring the export captures all visible results would improve accuracy. Adding a simple step to validate links before download or log failed attempts would also make the process more reliable and the results more consistent with what users see online.\n\n\n\nFor me, a personal area of improvement is developing a stronger understanding of how different R packages work. I spent a lot of time trying to use rvest because it was familiar, but the results should have been a clear sign that it wasn’t the right tool for this type of site. Once I realized that, the rest of the process, using the JSON export and adjusting the script, came together quickly. Recognizing those limitations earlier would have saved a lot of time and helped me move through the technical challenges more efficiently."
  },
  {
    "objectID": "assign05.html#report",
    "href": "assign05.html#report",
    "title": "Assignment 5 - Govt Data",
    "section": "",
    "text": "This assignment involved scraping data from the GovInfo website. I initially used the rvest package, which worked well for static sites like Wikipedia. However, I learned that rvest cannot interact with dynamic elements or trigger buttons that generate downloadable files. Because the GovInfo site requires exporting search results, this method could not be used to directly scrape the data.\n\n\nAfter reviewing the sample script, I realized the correct process was to export the search results in CSV or JSON format and then use R to download the linked PDFs. The CSV method only downloaded two PDFs, while switching to the JSON file improved the results but still didn’t reach the full number expected. Even after increasing the target from 10 to 20, only 14 files were downloaded.\nThe main difficulty was that the exported data did not fully match what appeared on the GovInfo website. Although most of the search results on the site showed a visible PDF button, some of those records either lacked a valid pdf link or contained empty fields in the export. This caused the script to skip certain rows or return fewer files than expected.\n\n\n\nThe exported data was usable as the files that were successfully downloaded all opened correctly as PDFs, showing that the links were valid. However, the exports included missing or invalid fields that limited the total number of successful downloads. The data provided enough structure, titles, dates, for further use, but would require cleanup and verification to ensure complete and accurate results.\n\n\n\nThe main area for improvement would be addressing the mismatch between the exported files and the actual search results displayed on the website. Cleaning the data to remove empty or invalid fields, verifying that each record includes a working link, and ensuring the export captures all visible results would improve accuracy. Adding a simple step to validate links before download or log failed attempts would also make the process more reliable and the results more consistent with what users see online.\n\n\n\nFor me, a personal area of improvement is developing a stronger understanding of how different R packages work. I spent a lot of time trying to use rvest because it was familiar, but the results should have been a clear sign that it wasn’t the right tool for this type of site. Once I realized that, the rest of the process, using the JSON export and adjusting the script, came together quickly. Recognizing those limitations earlier would have saved a lot of time and helped me move through the technical challenges more efficiently."
  },
  {
    "objectID": "assign06.html",
    "href": "assign06.html",
    "title": "Assignment 6 - Text Analytics",
    "section": "",
    "text": "Understanding the Word Clouds\nThe word clouds show the most frequent and distinctive words used in U.S. presidential inaugural speeches. Each cloud represents either an individual president or a broader time period. The larger the word, the more often it appears or the more unique it is to that speaker’s language.\nIn this case, you can see patterns in tone and focus. For example, words like “freedom” and “us” stand out in Obama’s speeches, while “America” and “great” stand out in Trump’s. Word clouds are great illustrations of how language and priorities shift over time.\n(These visuals are generated using the textplot_wordcloud() function from the quanteda.textplots package.)\n\n\n\nTop distinguishing terms in Post-2000 speeches\n\n\nfeature\nchi2\np\nn_target\nn_reference\n\n\n\n\namerica\n427.86752\n0\n107\n11\n\n\namerican\n178.46836\n0\n87\n57\n\n\ntoday\n170.39575\n0\n41\n3\n\n\nthank\n144.09955\n0\n34\n2\n\n\nfreedom\n85.89124\n0\n46\n34\n\n\nstori\n81.93645\n0\n19\n0\n\n\nknow\n72.67507\n0\n32\n18\n\n\nday\n71.87235\n0\n37\n26\n\n\nideal\n65.83894\n0\n19\n3\n\n\ntogeth\n65.37976\n0\n25\n11\n\n\n\n\n\n\nTop distinguishing terms in Pre-1900 speeches\n\n\nfeature\nchi2\np\nn_target\nn_reference\n\n\n\n\nus\n69.29860\n0.0e+00\n89\n140\n\n\nworld\n57.95143\n0.0e+00\n46\n51\n\n\nnew\n46.72204\n0.0e+00\n38\n43\n\n\npresid\n28.60372\n1.0e-07\n32\n46\n\n\nhistori\n27.43379\n2.0e-07\n25\n31\n\n\ncome\n25.39269\n5.0e-07\n20\n22\n\n\nstand\n23.08508\n1.5e-06\n21\n26\n\n\nnation\n22.08858\n2.6e-06\n103\n278\n\n\nend\n21.49257\n3.6e-06\n19\n23\n\n\npromis\n21.12440\n4.3e-06\n16\n17\n\n\n\n\n\n\nTop distinguishing terms in Democratic speeches\n\n\nfeature\nchi2\np\nn_target\nn_reference\n\n\n\n\ndemocraci\n36.33525\n0.0000000\n52\n17\n\n\nfellow\n20.13701\n0.0000072\n51\n28\n\n\nus\n19.99811\n0.0000078\n222\n220\n\n\npower\n18.19461\n0.0000199\n120\n103\n\n\nspirit\n17.80914\n0.0000244\n61\n40\n\n\nsacr\n15.25608\n0.0000939\n23\n8\n\n\nendur\n14.56338\n0.0001355\n25\n10\n\n\nhumbl\n13.45008\n0.0002450\n16\n4\n\n\nhappi\n11.45457\n0.0007132\n28\n15\n\n\nancient\n11.43971\n0.0007189\n13\n3\n\n\n\n\n\n\nTop distinguishing terms in Republican speeches\n\n\nfeature\nchi2\np\nn_target\nn_reference\n\n\n\n\neveri\n6.952140\n0.0083719\n109\n116\n\n\nlife\n5.048050\n0.0246537\n63\n64\n\n\nnew\n4.741566\n0.0294423\n113\n129\n\n\nshall\n3.884095\n0.0487455\n111\n130\n\n\nprincipl\n3.524817\n0.0604565\n41\n41\n\n\nthing\n3.438710\n0.0636854\n40\n40\n\n\ntoday\n3.300585\n0.0692552\n58\n63\n\n\nnation\n3.156100\n0.0756437\n252\n328\n\n\nchang\n3.080150\n0.0792532\n48\n51\n\n\nconfid\n2.899366\n0.0886144\n38\n39\n\n\n\n\n\n\nKWIC examples from early inaugural speeches (Washington 1789)\n\n\n\n\n\n\n\n\ndocname\nkeyword\npre\npost\n\n\n\n\n1789-Washington\nliberties\nbenediction may consecrate to the\nand happiness of the people\n\n\n1789-Washington\neconomy\nthat there exists in the\nand course of nature an\n\n\n1789-Washington\nunion\ncourse of nature an indissoluble\nbetween virtue and happiness ;\n\n\n1789-Washington\nliberties\nan arduous struggle for its\n, the light in which\n\n\n1789-Washington\nsecurity\nform of government for the\nof their union and the\n\n\n1789-Washington\nunion\nfor the security of their\nand the advancement of their\n\n\n\n\n\n\n\nSimilarities and Differences Over Time and Among Presidents\nOver time:\nEarly inaugural speeches (pre-1900) were grounded in the nation’s founding ideals and moral principles. The KWIC lines from Washington’s 1789 address highlight words like liberties, union, and economy, tied to virtue and happiness. The overall tone centered on morality, stability, and natural order.\nPost-2000 speeches, on the other hand, are filled with words like America, American, and today, pointing to a shift toward modern identity, global presence, and immediate political priorities. In short, presidential rhetoric moved from timeless ideals to a focus on action, identity, and leadership.\nAmong presidents and parties:\nDemocratic speeches often include words such as democraci, fellow, and us, showing a consistent emphasis on unity, inclusion, and collective progress.\nRepublican speeches, meanwhile, highlight terms like everi, life, principl, nation, and confidm; language that leans more on individual resolve, moral conviction, and national strength.\nBoth parties talk about progress and change, but their tones differ: Democrats tend to focus on shared purpose and optimism, while Republicans emphasize perseverance and foundational principles.\nOverall:\nAcross time and political lines, every president appeals to unity and national purpose. What changes is the lens, from liberty and virtue in the early years to modern themes of leadership and resilience today. The tone also shifts by party: Democrats are more collective and aspirational, while Republicans are more principled and motivational.\nNote: Words in the analysis appear in stemmed form (e.g., democraci for “democracy/democratic,” confid for “confidence,” everi for “every”). Stemming helps group related word forms together but may slightly reduce readability.\n\n\nWhat is Wordfish?\nFrom the Quanteda website, not much detail is given about what Wordfish actually is. To get a better grasp, I looked outside of Quanteda and found that Wordfish is a model that estimates where documents fall along a scale, like political leaning or tone, based on the words they use. It looks at how often certain words appear in each text and uses that to position them relative to one another. In short, it helps identify patterns or bias in language across different documents."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "For this assignment, I used the tidycensus package to pull 2023 ACS 5-year data at the county level in Texas. The goal was to create one map and one table that highlight different socioeconomic variables, followed by a brief interpretation of the results. I chose to examine the relationship between home value and commute time. My hypothesis was that those that commute less than 10 minutes will have a positive relationship with home values between $500-$749k.\n\nvars &lt;- c(\n  mort_500_749 = \"B25096_009\",\n  mort_total   = \"B25096_001\",\n  drive_u10    = \"B08534_022\",\n  drive_total  = \"B08534_001\")\n\ntx &lt;- get_acs(\n  geography = \"county\",\n  state = \"TX\",\n  variables = vars,\n  year = 2023,\n  survey = \"acs5\",\n  geometry = TRUE)\n\ntx_wide &lt;- tx |&gt;\n  select(GEOID, NAME, variable, estimate, moe, geometry) |&gt;\n  tidyr::pivot_wider(names_from = variable, values_from = c(estimate, moe)) |&gt;\n  mutate(\n    pct_mort_500_749 = ifelse(estimate_mort_total &gt; 0,\n                              100 * estimate_mort_500_749 / estimate_mort_total, NA_real_),\n    pct_drive_u10    = ifelse(estimate_drive_total &gt; 0,\n                              100 * estimate_drive_u10    / estimate_drive_total, NA_real_))\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 × 5\n   county           estimate_drive_u10 moe_drive_u10 total_workers pct_drive_u10\n   &lt;chr&gt;                         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Collingsworth C…                608           162          1045          58.2\n 2 Baylor County                   707           167          1261          56.1\n 3 Presidio County                1301           325          2483          52.4\n 4 Lamb County                    1862           271          3985          46.7\n 5 Pecos County                   2691           332          5818          46.3\n 6 Culberson County                622           218          1346          46.2\n 7 Floyd County                    783           153          1732          45.2\n 8 McCulloch County               1093           231          2423          45.1\n 9 Hansford County                1015           175          2283          44.5\n10 Coke County                     348           111           789          44.1\n\n\n# A tibble: 10 × 5\n   county           estimate_drive_u10 moe_drive_u10 total_workers pct_drive_u10\n   &lt;chr&gt;                         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n 1 Fort Bend County              15195          1364        200931           7.6\n 2 Montgomery Coun…              15773          1389        217492           7.3\n 3 El Paso County                25342          1484        347154           7.3\n 4 Collin County                 30109          1729        420218           7.2\n 5 Waller County                  1634           485         23928           6.8\n 6 Tarrant County                59866          2200        942163           6.4\n 7 Bexar County                  53908          2156        882863           6.1\n 8 Travis County                 36738          2053        683588           5.4\n 9 Dallas County                 72789          2500       1424038           5.1\n10 Harris County                111735          4027       2289694           4.9\n\n\n\n\nThe map shows the share of owner-occupied homes in Texas valued between $500,000 and $749,999. The lighter shades represent counties where this price range makes up a higher percentage of the housing stock. Notably, the San Antonio–Austin corridor (e.g., Bexar, Comal, Travis, Williamson) stands out more than Houston, indicating these counties have a relatively larger proportion of homes in this band.\nThe table complements the map by listing the counties with the highest and lowest proportions of short commute times (less than 10 minutes). Some counties have missing values because the sample sizes for those estimates were too small, but the table still highlights where short commutes are most and least common.\nWhen comparing these patterns, we can see that many of the counties with a higher share of $500k–$749k homes overlap with regions that are also among the wealthier parts of Texas. However, the idea of wealthiest counties depends on how wealth is defined, either by median income, total income, property values or a combination of all variables. For example, Harris County (Houston) is economically powerful overall but doesn’t show a large percentage of homes in this specific range because of its wider distribution of both lower- and higher-priced housing. In contrast, parts of Central Texas have a sharper concentration of homes in this bracket, suggesting affordability and demand align more tightly there.\nI selected these two variables because I hypothesized that there would be a positive relationship between commute time and home value, specifically, that the less time someone commutes, the higher their home value would be. The data pulled fails to reject this hypothesis."
  },
  {
    "objectID": "assign03.html#interpretation",
    "href": "assign03.html#interpretation",
    "title": "Assignment 3",
    "section": "",
    "text": "The map shows the share of owner-occupied homes in Texas valued between $500,000 and $749,999. The lighter shades represent counties where this price range makes up a higher percentage of the housing stock. Notably, the San Antonio–Austin corridor (e.g., Bexar, Comal, Travis, Williamson) stands out more than Houston, indicating these counties have a relatively larger proportion of homes in this band.\nThe table complements the map by listing the counties with the highest and lowest proportions of short commute times (less than 10 minutes). Some counties have missing values because the sample sizes for those estimates were too small, but the table still highlights where short commutes are most and least common.\nWhen comparing these patterns, we can see that many of the counties with a higher share of $500k–$749k homes overlap with regions that are also among the wealthier parts of Texas. However, the idea of wealthiest counties depends on how wealth is defined, either by median income, total income, property values or a combination of all variables. For example, Harris County (Houston) is economically powerful overall but doesn’t show a large percentage of homes in this specific range because of its wider distribution of both lower- and higher-priced housing. In contrast, parts of Central Texas have a sharper concentration of homes in this bracket, suggesting affordability and demand align more tightly there.\nI selected these two variables because I hypothesized that there would be a positive relationship between commute time and home value, specifically, that the less time someone commutes, the higher their home value would be. The data pulled fails to reject this hypothesis."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Below you’ll find a selection of projects and assignments that demonstrate my work with Geographic Information Systems (GIS) software, including ArcGIS Online and ArcGIS Pro. These projects highlight skills in spatial data analysis, visualization, and storytelling with maps. Over time, this page will also feature work from my Methods of Data Collection and Production course, showcasing additional approaches to data gathering, analysis, and presentation."
  },
  {
    "objectID": "projects.html#gis-mapping",
    "href": "projects.html#gis-mapping",
    "title": "Projects",
    "section": "GIS & Mapping",
    "text": "GIS & Mapping\n\nUS Population Change StoryMap\nA narrative of population change with interactive maps and visuals.\nhttps://arcg.is/1L9Gy80\nHealthcare Accessibility in the DFW\nAn interactive web app built with ArcGIS Experience Builder that examines healthcare accessibility in the Dallas–Fort Worth area by visualizing distance to Medicare-registered hospitals alongside median household income and the Social Vulnerability Index as a mediating factor.\nhttps://experience.arcgis.com/experience/9f4a7b9e752c455f8873e58818918f3c"
  },
  {
    "objectID": "projects.html#methods-data-coming-soon",
    "href": "projects.html#methods-data-coming-soon",
    "title": "Projects",
    "section": "Methods & Data (coming soon)",
    "text": "Methods & Data (coming soon)"
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Website Design Note\nThis website was created using both RStudio and Quarto. The default theme is “cosmo,” but I elected to use the “brand” theme, not for any functional reason, but simply after trying out a few options I found through a Google search. I landed on this one because I’m hopeful it stands out from other students’ sites while still maintaining professionalism.\nThe navigation bar includes five links that make it easy for visitors to explore the site’s content. The Home page greets visitors with a general introduction. The About section gives more detail about my background and research interests. The Resume link provides direct access to a PDF of my most current resume. Finally, the Projects and Assignments links will mainly include coursework from data-related and GIS classes, starting with this first assignment, a reflection on the design of this website.\nAltogether, this website serves two purposes, to become a portfolio of my coursework and eventually, a foundation to showcase professional projects in the future.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]