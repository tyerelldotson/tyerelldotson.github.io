```{r, include=FALSE}

library(rvest)
library(dplyr)
library(stringr)
library(readr)
library(tidyr)
library(janitor)
library(httr2)
```

---
title: "Assignment 4"
---

# Web Scraping

```{r}
url_fx <- "https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves"

fx_raw <- read_html(url_fx) |>
  html_elements("table.wikitable") |>
  (\(x) html_table(x, fill = TRUE))() |>
  (\(x) x[[1]])()

fx <- fx_raw |>
  janitor::remove_empty(c("rows","cols")) |>
  janitor::clean_names()

# safer numeric parser: make non-digit cells NA before parsing
to_num <- function(x) {
  x <- as.character(x)
  x <- gsub("[^0-9\\-\\.]", "", x)  # strip commas, spaces, footnotes, symbols
  x[!grepl("\\d", x)] <- NA         # if no digits at all, set NA
  suppressWarnings(readr::parse_number(x))
}

fx_clean <- fx |>
  dplyr::transmute(
    country   = country_as_recognized_by_the_u_n,
    continent = continent,
    reserves_including_gold_musd = to_num(foreign_exchange_reserves),
    reserves_excluding_gold_musd = to_num(foreign_exchange_reserves_3),
    as_of = Sys.Date()
  ) |>
  dplyr::filter(
    !is.na(country), country != "",
    !is.na(reserves_including_gold_musd) | !is.na(reserves_excluding_gold_musd)
  ) |>
  head(15)

fx_clean
```

```{r}
## Diamond production (Wikipedia)

url_diam <- "https://en.wikipedia.org/wiki/List_of_countries_by_diamond_production"

# read page and grab only Wikipedia data tables
doc <- read_html(url_diam)
tbls <- doc |> html_elements("table.wikitable") |> html_table(fill = TRUE)

# choose the largest wikitable (robust across minor page edits)
diam_raw <- tbls[[ which.max(sapply(tbls, nrow) + sapply(tbls, ncol)) ]]

# clean and standardize
diam <- diam_raw |>
  janitor::remove_empty(c("rows","cols")) |>
  janitor::clean_names()

# --- map columns deterministically but without hard-coding names ---
# assume first col = country, second col = production (carats or similar)
# (If the page changes, run `names(diam)` once and tweak the two picks.)
country_col  <- names(diam)[1]
produce_col  <- names(diam)[2]

diam <- diam |>
  dplyr::transmute(
    country = .data[[country_col]],
    production_raw = .data[[produce_col]],
    production_carat = readr::parse_number(gsub(",", "", production_raw)),
    as_of = Sys.Date()
  ) |>
  dplyr::filter(
    !is.na(country), country != "",
    !grepl("world|total|region|rank", tolower(country))
  )

# quick look
head(diam, 15)
```

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_countries_by_pharmaceutical_exports"
page <- read_html(url)

# get all wikitables + their captions
tabs <- page |> html_elements("table.wikitable")
caps <- vapply(tabs, \(n) { cap <- html_element(n, "caption"); if (length(cap)) html_text2(cap) else NA_character_ }, "")

# pick the table whose caption mentions "pharmaceutical exports"
i <- which(grepl("pharmaceutical exports", tolower(caps)))[1]
stopifnot(!is.na(i))

ph <- tabs[[i]] |> html_table(fill = TRUE) |>
  janitor::remove_empty(c("rows","cols")) |>
  janitor::clean_names()

# confirm names you’ll see here (typically: country, value_exported_thousands_usd, ...)
# names(ph)

# keep Country + Value exported, parse number, first 15 rows
clean_num <- function(x) readr::parse_number(gsub("[^0-9\\-\\.]", "", as.character(x)))
ph_exports <- ph |>
  dplyr::transmute(
    country,
    exports_usd_thousands = clean_num(value_exported_thousands_usd),
    as_of = Sys.Date()
  ) |>
  dplyr::filter(!grepl("^world$|^total$", tolower(country))) |>
  head(15)

ph_exports
```

# Data Plan

If I were to acquire web data for research in the future, I would develop a data plan focused on ensuring accuracy, transparency, and ethical collection practices. This would include identifying reliable and authoritative websites, databases, or APIs that allow access to relevant information. I would also prioritize reproducibility by clearly documenting the data sources, collection methods, and any transformations made during cleaning or analysis. When using web scraping, I would comply with site policies, minimize bias, and verify the data’s validity before incorporating it into the study. The goal would be to gather structured data that supports the research question and can withstand academic scrutiny.
